{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMr94iqHjd9cz3edM55XUe4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import triton\n",
        "import triton.language as tl"
      ],
      "metadata": {
        "id": "1VrQfXTpDSUW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def _attn_fwd_inner(\n",
        "    O_block,\n",
        "    l_i,\n",
        "    m_i,\n",
        "    Q_block,\n",
        "    K_block_ptr,\n",
        "    V_block_ptr,\n",
        "    block_index_q,\n",
        "    softmax_scale,\n",
        "    BLOCK_SIZE_Q: tl.constexpr,\n",
        "    BLOCK_SIZE_KV: tl.constexpr,\n",
        "    STAGE: tl.constexpr,\n",
        "    offs_q: tl.constexpr,\n",
        "    offs_kv: tl.constexpr,\n",
        "    SEQ_LEN: tl.constexpr,\n",
        "):\n",
        "    # range of values handled by this stage\n",
        "    if STAGE == 1:\n",
        "        # From 0 to the left of the diagonal\n",
        "        lo, hi = 0, block_index_q * BLOCK_SIZE_Q\n",
        "    elif STAGE == 2:\n",
        "        # Used only for the block in which there is transition between non-masked and masked keys\n",
        "        lo, hi = block_index_q * BLOCK_SIZE_Q, (block_index_q + 1) * BLOCK_SIZE_Q\n",
        "        lo = tl.multiple_of(lo, BLOCK_SIZE_Q)\n",
        "    else:\n",
        "        # For non-causal attention\n",
        "        lo, hi = 0, SEQ_LEN\n",
        "\n",
        "    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n",
        "    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n",
        "\n",
        "    # loop over k, v and update accumulator\n",
        "    for start_kv in range(lo, hi, BLOCK_SIZE_KV):\n",
        "        # Let the compiler know that start_n is a multiple of BLOCK_N for compiler optimization\n",
        "        start_kv = tl.multiple_of(start_kv, BLOCK_SIZE_KV)\n",
        "\n",
        "        # -- compute qk ----\n",
        "        K_block = tl.load(K_block_ptr)\n",
        "        QK_block = tl.dot(Q_block, K_block)\n",
        "\n",
        "        if STAGE == 2:\n",
        "            mask = offs_q[:, None] >= (start_kv + offs_kv[None, :])\n",
        "            QK_block = QK_block * softmax_scale + tl.where(mask, 0, -1.0e6)\n",
        "            m_ij = tl.maximum(m_i, tl.max(QK_block, 1))\n",
        "            QK_block -= m_ij[:, None]\n",
        "        else:\n",
        "            # Compute the maximum value of qk\n",
        "            m_ij = tl.maximum(m_i, tl.max(QK_block, 1) * softmax_scale)\n",
        "            QK_block = QK_block * softmax_scale - m_ij[:, None]\n",
        "\n",
        "        # Compute the exponential of each dot product, so -> exp(qk_ij - m_ij)\n",
        "        P_block = tl.math.exp(QK_block)\n",
        "        # Compute the sum by rows of the attention scores\n",
        "        l_ij = tl.sum(P_block, 1)\n",
        "\n",
        "        # Correction factor for the previous l_i\n",
        "        alpha = tl.math.exp(m_i - m_ij)\n",
        "        # Apply the correction factor to the previous l_i and add the new l_ij\n",
        "        l_i = l_i * alpha + l_ij\n",
        "\n",
        "        V_block = tl.load(V_block_ptr)\n",
        "        P_block = P_block.to(tl.float16)\n",
        "        #O_new = P x V + O_old * alpha\n",
        "        O_block = O_block * alpha[:, None]\n",
        "        O_block = tl.dot(P_block, V_block, O_block)\n",
        "\n",
        "        m_i = m_ij\n",
        "\n",
        "        # Next block of K and V\n",
        "        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_SIZE_KV, 0))\n",
        "        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_SIZE_KV))\n",
        "    return O_block, l_i, m_i\n"
      ],
      "metadata": {
        "id": "Lrme-XEDDT-8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.autotune(\n",
        "    [\n",
        "        triton.Config(\n",
        "            {\"BLOCK_SIZE_Q\": BLOCK_SIZE_Q, \"BLOCK_SIZE_KV\": BLOCK_SIZE_KV},\n",
        "            num_stages=num_stages,\n",
        "            num_warps=num_warps,\n",
        "        )\n",
        "        for BLOCK_SIZE_Q in [64, 128]\n",
        "        for BLOCK_SIZE_KV in [32, 64]\n",
        "        for num_stages in ([3, 4, 7])\n",
        "        for num_warps in [2, 4]\n",
        "    ],\n",
        "    key=[\"SEQ_LEN\", \"HEAD_DIM\"],\n",
        ")\n",
        "@triton.jit\n",
        "def _attn_fwd(\n",
        "    Q,  # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM\n",
        "    K,  # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM\n",
        "    V,  # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM\n",
        "    softmax_scale,\n",
        "    M,  # BATCH_SIZE, NUM_HEADS, SEQ_LEN\n",
        "    O,  # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM\n",
        "    stride_Q_batch,\n",
        "    stride_Q_head,\n",
        "    stride_Q_seq,\n",
        "    stride_Q_dim,\n",
        "    stride_K_batch,\n",
        "    stride_K_head,\n",
        "    stride_K_seq,\n",
        "    stride_K_dim,\n",
        "    stride_V_batch,\n",
        "    stride_V_head,\n",
        "    stride_V_seq,\n",
        "    stride_V_dim,\n",
        "    stride_O_batch,\n",
        "    stride_O_head,\n",
        "    stride_O_seq,\n",
        "    stride_O_dim,\n",
        "    BATCH_SIZE,\n",
        "    NUM_HEADS: tl.constexpr,\n",
        "    SEQ_LEN: tl.constexpr,\n",
        "    HEAD_DIM: tl.constexpr,\n",
        "    BLOCK_SIZE_Q: tl.constexpr,\n",
        "    BLOCK_SIZE_KV: tl.constexpr,\n",
        "    STAGE: tl.constexpr,\n",
        "):\n",
        "    tl.static_assert(BLOCK_SIZE_KV <= HEAD_DIM)\n",
        "\n",
        "    #block in the sequence length to process\n",
        "    block_index_q = tl.program_id(0)\n",
        "\n",
        "    #head and batch to process. Each program is associated with a single head of a single batch\n",
        "    index_batch_head = tl.program_id(1)\n",
        "    #batch this program is associated with (each batch has NUM_HEADS heads)\n",
        "    index_batch = index_batch_head // NUM_HEADS\n",
        "    #the position of the head in the batch\n",
        "    index_head = index_batch_head % NUM_HEADS\n",
        "\n",
        "    #Get the (N_CTX, HEAD_DIM) block in the Q, K, V by selecting indexing it by batch and head\n",
        "    qvk_offset = (\n",
        "        index_batch.to(tl.int64) * stride_Q_batch\n",
        "        + index_head.to(tl.int64) * stride_Q_head\n",
        "    )\n",
        "\n",
        "    Q_block_ptr = tl.make_block_ptr(\n",
        "        base=Q + qvk_offset,\n",
        "        shape=(SEQ_LEN, HEAD_DIM),\n",
        "        strides=(stride_Q_seq, stride_Q_dim),\n",
        "        offsets=(block_index_q * BLOCK_SIZE_Q, 0),\n",
        "        block_shape=(BLOCK_SIZE_Q, HEAD_DIM),\n",
        "        order=(1, 0),\n",
        "    )\n",
        "\n",
        "    V_block_ptr = tl.make_block_ptr(\n",
        "        base=V + qvk_offset,\n",
        "        shape=(SEQ_LEN, HEAD_DIM),\n",
        "        strides=(stride_V_seq, stride_V_dim),\n",
        "        offsets=(0, 0),\n",
        "        block_shape=(BLOCK_SIZE_KV, HEAD_DIM),\n",
        "        order=(1, 0),\n",
        "    )\n",
        "\n",
        "    K_block_ptr = tl.make_block_ptr(\n",
        "        base=K + qvk_offset,\n",
        "        shape=(HEAD_DIM, SEQ_LEN),\n",
        "        strides=(\n",
        "            stride_K_dim,\n",
        "            stride_K_seq,\n",
        "        ),  # We invert the strides w.r.t Q\n",
        "        offsets=(0, 0),\n",
        "        block_shape=(HEAD_DIM, BLOCK_SIZE_KV),\n",
        "        order=(0, 1),\n",
        "    )\n",
        "\n",
        "    O_block_ptr = tl.make_block_ptr(\n",
        "        base=O + qvk_offset,\n",
        "        shape=(SEQ_LEN, HEAD_DIM),\n",
        "        strides=(stride_O_seq, stride_O_dim),\n",
        "        offsets=(block_index_q * BLOCK_SIZE_Q, 0),\n",
        "        block_shape=(BLOCK_SIZE_Q, HEAD_DIM),\n",
        "        order=(1, 0),\n",
        "    )\n",
        "\n",
        "    # offs_q: the offsets for the tokens in the Q to process\n",
        "    offs_q = block_index_q * BLOCK_SIZE_Q + tl.arange(0, BLOCK_SIZE_Q)\n",
        "    # offs_kv: the offsets for the tokens in the K and V sequence to process\n",
        "    offs_kv = tl.arange(0, BLOCK_SIZE_KV)\n",
        "\n",
        "    # m_i: the running maximum\n",
        "    m_i = tl.zeros([BLOCK_SIZE_Q], dtype=tl.float32) - float(\"inf\")\n",
        "    # l_i: the running sum\n",
        "    l_i = tl.zeros([BLOCK_SIZE_Q], dtype=tl.float32) + 1.0\n",
        "    # acc: the accumulator for the output\n",
        "    O_block = tl.zeros([BLOCK_SIZE_Q, HEAD_DIM], dtype=tl.float32)\n",
        "\n",
        "    # load the blocks of Q\n",
        "    Q_block = tl.load(Q_block_ptr)\n",
        "\n",
        "\n",
        "\n",
        "    if STAGE == 1 or STAGE == 3:\n",
        "        # non-causal attention or for the blocks to the left of the diagonal\n",
        "        O_block, l_i, m_i = _attn_fwd_inner(\n",
        "            O_block,\n",
        "            l_i,\n",
        "            m_i,\n",
        "            Q_block,\n",
        "            K_block_ptr,\n",
        "            V_block_ptr,\n",
        "            block_index_q,\n",
        "            softmax_scale,\n",
        "            BLOCK_SIZE_Q,\n",
        "            BLOCK_SIZE_KV,\n",
        "            4 - STAGE,\n",
        "            offs_q,\n",
        "            offs_kv,\n",
        "            SEQ_LEN,\n",
        "        )\n",
        "\n",
        "    if STAGE == 3:\n",
        "        # Blocks to the right of the diagonal in the causal attention\n",
        "        O_block, l_i, m_i = _attn_fwd_inner(\n",
        "            O_block,\n",
        "            l_i,\n",
        "            m_i,\n",
        "            Q_block,\n",
        "            K_block_ptr,\n",
        "            V_block_ptr,\n",
        "            block_index_q,\n",
        "            softmax_scale,\n",
        "            BLOCK_SIZE_Q,\n",
        "            BLOCK_SIZE_KV,\n",
        "            2,\n",
        "            offs_q,\n",
        "            offs_kv,\n",
        "            SEQ_LEN,\n",
        "        )\n",
        "\n",
        "    m_i += tl.math.log(\n",
        "        l_i\n",
        "    )  # logsumexp for the backwards pass\n",
        "    O_block = O_block / l_i[:, None]\n",
        "    m_ptrs = M + index_batch_head * SEQ_LEN + offs_q\n",
        "    tl.store(m_ptrs, m_i)\n",
        "    tl.store(O_block_ptr, O_block.to(O.type.element_ty))"
      ],
      "metadata": {
        "id": "xhLYgrEZDWB7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def _attn_bwd_preprocess(\n",
        "    O,\n",
        "    dO,\n",
        "    D,\n",
        "    SEQ_LEN,\n",
        "    BLOCK_SIZE_Q: tl.constexpr,\n",
        "    HEAD_DIM: tl.constexpr,\n",
        "):\n",
        "    block_index_q = tl.program_id(0)\n",
        "    offs_q = block_index_q * BLOCK_SIZE_Q + tl.arange(0, BLOCK_SIZE_Q)\n",
        "    index_batch_head = tl.program_id(1)\n",
        "    offs_dim = tl.arange(0, HEAD_DIM)\n",
        "    # Load single block of BLOCK_SIZE_Q rows of O\n",
        "    O_block = tl.load(\n",
        "        O\n",
        "        + index_batch_head * HEAD_DIM * SEQ_LEN\n",
        "        + offs_q[:, None] * HEAD_DIM\n",
        "        + offs_dim[None, :]\n",
        "    )\n",
        "    # Load single block of BLOCK_SIZE_Q rows of dO\n",
        "    dO_block = tl.load(\n",
        "        dO\n",
        "        + index_batch_head * HEAD_DIM * SEQ_LEN\n",
        "        + offs_q[:, None] * HEAD_DIM\n",
        "        + offs_dim[None, :]\n",
        "    ).to(tl.float32)\n",
        "    # Compute the D block\n",
        "    D_block = tl.sum(dO_block * O_block, axis=1)  # Shape: (BLOCK_SIZE_Q,)\n",
        "    # Store the D block\n",
        "    D_block_ptrs = D + index_batch_head * SEQ_LEN + offs_q\n",
        "    tl.store(D_block_ptrs, D_block)\n"
      ],
      "metadata": {
        "id": "zlQsxTlNDY38"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def _attn_bwd_dq(\n",
        "    Q,\n",
        "    K,\n",
        "    V,\n",
        "    softmax_scale,\n",
        "    dO,\n",
        "    dQ,\n",
        "    dK,\n",
        "    dV,\n",
        "    M,\n",
        "    D,\n",
        "    stride_batch,\n",
        "    stride_head,\n",
        "    stride_seq,\n",
        "    stride_dim,\n",
        "    NUM_HEADS,\n",
        "    SEQ_LEN,\n",
        "    BLOCK_Q: tl.constexpr,\n",
        "    BLOCK_KV: tl.constexpr,\n",
        "    HEAD_DIM: tl.constexpr,\n",
        "    STAGE: tl.constexpr,\n",
        "):\n",
        "    index_batch_head = tl.program_id(2)\n",
        "    index_batch = index_batch_head // NUM_HEADS\n",
        "    index_head = index_batch_head % NUM_HEADS\n",
        "    offset_batch_head = (stride_batch * index_batch + stride_head * index_head).to(\n",
        "        tl.int64\n",
        "    )\n",
        "    #Offset that allows to select the right sequence\n",
        "    offset_batch_head_seq = (index_batch_head * SEQ_LEN).to(tl.int64)\n",
        "\n",
        "\n",
        "    Q += offset_batch_head\n",
        "    K += offset_batch_head\n",
        "    V += offset_batch_head\n",
        "    dO += offset_batch_head\n",
        "    dQ += offset_batch_head\n",
        "    dK += offset_batch_head\n",
        "    dV += offset_batch_head\n",
        "\n",
        "    #pointers are in the right place w.r.t batch, head and sequence\n",
        "    M += offset_batch_head_seq\n",
        "    D += offset_batch_head_seq\n",
        "\n",
        "    # load scales\n",
        "    offs_dim = tl.arange(0, HEAD_DIM)\n",
        "\n",
        "    index_block_kv = tl.program_id(0)\n",
        "\n",
        "    start_q = index_block_kv * BLOCK_Q\n",
        "    offs_q = start_q + tl.arange(0, BLOCK_Q)\n",
        "\n",
        "    Q_block = tl.load(Q + offs_q[:, None] * stride_seq + offs_dim[None, :] * stride_dim)\n",
        "    dQ_block = tl.zeros([BLOCK_Q, HEAD_DIM], dtype=tl.float32)\n",
        "    dO_block = tl.load(\n",
        "        dO + offs_q[:, None] * stride_seq + offs_dim[None, :] * stride_dim\n",
        "    )\n",
        "\n",
        "    M_block = tl.load(M + offs_q)\n",
        "    M_block = M_block[:, None]\n",
        "\n",
        "    offs_kv = tl.arange(0, BLOCK_KV)\n",
        "\n",
        "    #access the K and V as transposed blocks\n",
        "    kT_ptrs = K + offs_kv[None, :] * stride_seq + offs_dim[:, None] * stride_dim\n",
        "    vT_ptrs = V + offs_kv[None, :] * stride_seq + offs_dim[:, None] * stride_dim\n",
        "\n",
        "    Di = tl.load(D + offs_q)\n",
        "\n",
        "    curr_kv = 0\n",
        "    num_steps = SEQ_LEN // BLOCK_KV\n",
        "    for blk_idx in range(num_steps):\n",
        "        K_T_block = tl.load(kT_ptrs)\n",
        "        V_T_block = tl.load(vT_ptrs)\n",
        "        QK_block = softmax_scale * tl.dot(Q_block, K_T_block)\n",
        "        P_block = tl.math.exp(QK_block - M_block)\n",
        "\n",
        "        if STAGE == 3:\n",
        "            # Autoregressive masking.\n",
        "            offs_kv = curr_kv + tl.arange(0, BLOCK_KV)\n",
        "            mask_block = offs_q[:, None] >= offs_kv[None, :]\n",
        "            P_block = tl.where(mask_block, P_block, 0.0)\n",
        "\n",
        "        # Compute dP and dS.\n",
        "        dP_block = tl.dot(dO_block, V_T_block).to(tl.float32)\n",
        "        dS_block = P_block * (dP_block - Di[:, None])\n",
        "        dS_block = dS_block.to(tl.float16)\n",
        "        # Compute dQ.\n",
        "\n",
        "        dQ_block += softmax_scale * tl.dot(dS_block, tl.trans(K_T_block))\n",
        "        # Increment pointers.\n",
        "        curr_kv += BLOCK_KV\n",
        "        kT_ptrs += BLOCK_KV * stride_seq\n",
        "        vT_ptrs += BLOCK_KV * stride_seq\n",
        "\n",
        "    dQ_block_ptrs = dQ + offs_q[:, None] * stride_seq + offs_dim[None, :] * stride_dim\n",
        "    tl.store(dQ_block_ptrs, dQ_block)\n"
      ],
      "metadata": {
        "id": "-vpSVU_lDccZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def _attn_bwd_dk_dv(\n",
        "    Q,\n",
        "    K,\n",
        "    V,\n",
        "    softmax_scale,\n",
        "    dO,\n",
        "    dQ,\n",
        "    dK,\n",
        "    dV,\n",
        "    M,\n",
        "    D,\n",
        "    stride_batch,\n",
        "    stride_head,\n",
        "    stride_seq,\n",
        "    stride_dim,\n",
        "    NUM_HEADS,\n",
        "    SEQ_LEN,\n",
        "    BLOCK_Q: tl.constexpr,\n",
        "    BLOCK_KV: tl.constexpr,\n",
        "    HEAD_DIM: tl.constexpr,\n",
        "    STAGE: tl.constexpr,\n",
        "):\n",
        "    index_batch_head = tl.program_id(2)\n",
        "    index_batch = index_batch_head // NUM_HEADS\n",
        "    index_head = index_batch_head % NUM_HEADS\n",
        "    offset_batch_head = (stride_batch * index_batch + stride_head * index_head).to(\n",
        "        tl.int64\n",
        "    )\n",
        "    #Offset that allows us to select the right sequence given the batch and head.\n",
        "    offset_batch_head_seq = (index_batch_head * SEQ_LEN).to(tl.int64)\n",
        "\n",
        "\n",
        "    Q += offset_batch_head\n",
        "    K += offset_batch_head\n",
        "    V += offset_batch_head\n",
        "    dO += offset_batch_head\n",
        "    dQ += offset_batch_head\n",
        "    dK += offset_batch_head\n",
        "    dV += offset_batch_head\n",
        "\n",
        "\n",
        "    M += offset_batch_head_seq\n",
        "    D += offset_batch_head_seq\n",
        "\n",
        "    # load scales\n",
        "    offs_dim = tl.arange(0, HEAD_DIM)\n",
        "\n",
        "    index_block_kv = tl.program_id(0)\n",
        "    start_kv = index_block_kv * BLOCK_KV\n",
        "\n",
        "    offs_kv = start_kv + tl.arange(0, BLOCK_KV)\n",
        "\n",
        "    dV_block = tl.zeros([BLOCK_KV, HEAD_DIM], dtype=tl.float32)\n",
        "    dK_block = tl.zeros([BLOCK_KV, HEAD_DIM], dtype=tl.float32)\n",
        "\n",
        "    # load K and V:\n",
        "    K_block = tl.load(\n",
        "        K + offs_kv[:, None] * stride_seq + offs_dim[None, :] * stride_dim\n",
        "    )  # Shape: (BLOCK_KV1, HEAD_DIM)\n",
        "    V_block = tl.load(\n",
        "        V + offs_kv[:, None] * stride_seq + offs_dim[None, :] * stride_dim\n",
        "    )  # Shape: (BLOCK_KV1, HEAD_DIM)\n",
        "\n",
        "    offs_q = tl.arange(0, BLOCK_Q)\n",
        "\n",
        "\n",
        "    qT_ptrs = Q + offs_q[None, :] * stride_seq + offs_dim[:, None] * stride_dim\n",
        "    dO_ptrs = dO + offs_q[:, None] * stride_seq + offs_dim[None, :] * stride_dim\n",
        "\n",
        "    # Iterates over the sequence dimension of the query\n",
        "    curr_q = 0\n",
        "    num_steps = SEQ_LEN // BLOCK_Q\n",
        "    for blk_idx in range(num_steps):\n",
        "        # Load a block of Q\n",
        "        qT_block = tl.load(qT_ptrs)\n",
        "        # Load the logsumexp values\n",
        "        offs_q = curr_q + tl.arange(0, BLOCK_Q)\n",
        "        m = tl.load(M + offs_q)\n",
        "\n",
        "        #(QK^T)^T = (K^T)^T(Q^T) = K(Q^T) = P^T\n",
        "        QK_T_block = softmax_scale * tl.dot(K_block, qT_block)\n",
        "        #Apply softmax\n",
        "        P_T_block = tl.math.exp(QK_T_block - m[None, :])\n",
        "\n",
        "        if STAGE == 3:\n",
        "            # Autoregressive masking.\n",
        "            # mask is True for all values that DO NOT NEED TO BE MASKED\n",
        "            mask_block = (\n",
        "                offs_q[None, :] >= offs_kv[:, None]\n",
        "            )  # Shape: (BLOCK_KV1, BLOCK_Q1)\n",
        "\n",
        "            P_T_block = tl.where(mask_block, P_T_block, 0.0)\n",
        "\n",
        "        dO_block = tl.load(dO_ptrs)\n",
        "        #dV_new = dV_old + P^T x dO, where x is the matrix multiplication\n",
        "        dV_block += tl.dot(P_T_block.to(tl.float16), dO_block)\n",
        "\n",
        "        # Delta = rowsum(O * dO)\n",
        "        Di = tl.load(D + offs_q)\n",
        "\n",
        "        # dP = dO x V^T, so dP^T = V x dO^T\n",
        "\n",
        "        dpT_block = tl.dot(V_block, tl.trans(dO_block)).to(tl.float32)\n",
        "\n",
        "        #dS = P * (dP - Delta), so dS^T = P^T * (dP^T - Delta^T)\n",
        "\n",
        "        dS_T_block = P_T_block * (dpT_block - Di[None, :])\n",
        "        dS_T_block = dS_T_block.to(tl.float16)\n",
        "\n",
        "        #dK_new = dK_old + dS^T x Q\n",
        "        dK_block += softmax_scale * tl.dot(dS_T_block, tl.trans(qT_block))\n",
        "        # Increment pointers.\n",
        "        curr_q += BLOCK_Q\n",
        "        qT_ptrs += BLOCK_Q * stride_seq\n",
        "        dO_ptrs += BLOCK_Q * stride_seq\n",
        "\n",
        "    # Write back dV.\n",
        "    dV_block_ptrs = dV + offs_kv[:, None] * stride_seq + offs_dim[None, :] * stride_dim\n",
        "    tl.store(dV_block_ptrs, dV_block)\n",
        "\n",
        "    # Write back dK.\n",
        "    dK_block_ptrs = dK + offs_kv[:, None] * stride_seq + offs_dim[None, :] * stride_dim\n",
        "    tl.store(dK_block_ptrs, dK_block)\n"
      ],
      "metadata": {
        "id": "IsL2V9mkDd2V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def _attn_bwd_dk_dv(\n",
        "    Q,\n",
        "    K,\n",
        "    V,\n",
        "    softmax_scale,\n",
        "    dO,\n",
        "    dQ,\n",
        "    dK,\n",
        "    dV,\n",
        "    M,\n",
        "    D,\n",
        "    stride_batch,\n",
        "    stride_head,\n",
        "    stride_seq,\n",
        "    stride_dim,\n",
        "    NUM_HEADS,\n",
        "    SEQ_LEN,\n",
        "    BLOCK_Q: tl.constexpr,\n",
        "    BLOCK_KV: tl.constexpr,\n",
        "    HEAD_DIM: tl.constexpr,\n",
        "    STAGE: tl.constexpr,\n",
        "):\n",
        "    index_batch_head = tl.program_id(2)\n",
        "    index_batch = index_batch_head // NUM_HEADS\n",
        "    index_head = index_batch_head % NUM_HEADS\n",
        "    offset_batch_head = (stride_batch * index_batch + stride_head * index_head).to(\n",
        "        tl.int64\n",
        "    )\n",
        "    #Offset that allows us to select the right sequence given the batch and head.\n",
        "    offset_batch_head_seq = (index_batch_head * SEQ_LEN).to(tl.int64)\n",
        "\n",
        "\n",
        "    Q += offset_batch_head\n",
        "    K += offset_batch_head\n",
        "    V += offset_batch_head\n",
        "    dO += offset_batch_head\n",
        "    dQ += offset_batch_head\n",
        "    dK += offset_batch_head\n",
        "    dV += offset_batch_head\n",
        "\n",
        "\n",
        "    M += offset_batch_head_seq\n",
        "    D += offset_batch_head_seq\n",
        "\n",
        "    # load scales\n",
        "    offs_dim = tl.arange(0, HEAD_DIM)\n",
        "\n",
        "    index_block_kv = tl.program_id(0)\n",
        "    start_kv = index_block_kv * BLOCK_KV\n",
        "\n",
        "    offs_kv = start_kv + tl.arange(0, BLOCK_KV)\n",
        "\n",
        "    dV_block = tl.zeros([BLOCK_KV, HEAD_DIM], dtype=tl.float32)\n",
        "    dK_block = tl.zeros([BLOCK_KV, HEAD_DIM], dtype=tl.float32)\n",
        "\n",
        "    # load K and V:\n",
        "    K_block = tl.load(\n",
        "        K + offs_kv[:, None] * stride_seq + offs_dim[None, :] * stride_dim\n",
        "    )  # Shape: (BLOCK_KV1, HEAD_DIM)\n",
        "    V_block = tl.load(\n",
        "        V + offs_kv[:, None] * stride_seq + offs_dim[None, :] * stride_dim\n",
        "    )  # Shape: (BLOCK_KV1, HEAD_DIM)\n",
        "\n",
        "    offs_q = tl.arange(0, BLOCK_Q)\n",
        "\n",
        "\n",
        "    qT_ptrs = Q + offs_q[None, :] * stride_seq + offs_dim[:, None] * stride_dim\n",
        "    dO_ptrs = dO + offs_q[:, None] * stride_seq + offs_dim[None, :] * stride_dim\n",
        "\n",
        "    # Iterates over the sequence dimension of the query\n",
        "    curr_q = 0\n",
        "    num_steps = SEQ_LEN // BLOCK_Q\n",
        "    for blk_idx in range(num_steps):\n",
        "        # Load a block of Q\n",
        "        qT_block = tl.load(qT_ptrs)\n",
        "        # Load the logsumexp values\n",
        "        offs_q = curr_q + tl.arange(0, BLOCK_Q)\n",
        "        m = tl.load(M + offs_q)\n",
        "\n",
        "        #(QK^T)^T = (K^T)^T(Q^T) = K(Q^T) = P^T\n",
        "        QK_T_block = softmax_scale * tl.dot(K_block, qT_block)\n",
        "        #Apply softmax\n",
        "        P_T_block = tl.math.exp(QK_T_block - m[None, :])\n",
        "\n",
        "        if STAGE == 3:\n",
        "            # Autoregressive masking.\n",
        "            # mask is True for all values that DO NOT NEED TO BE MASKED\n",
        "            mask_block = (\n",
        "                offs_q[None, :] >= offs_kv[:, None]\n",
        "            )  # Shape: (BLOCK_KV1, BLOCK_Q1)\n",
        "\n",
        "            P_T_block = tl.where(mask_block, P_T_block, 0.0)\n",
        "\n",
        "        dO_block = tl.load(dO_ptrs)\n",
        "        #dV_new = dV_old + P^T x dO, where x is the matrix multiplication\n",
        "        dV_block += tl.dot(P_T_block.to(tl.float16), dO_block)\n",
        "\n",
        "        # Delta = rowsum(O * dO)\n",
        "        Di = tl.load(D + offs_q)\n",
        "\n",
        "        # dP = dO x V^T, so dP^T = V x dO^T\n",
        "\n",
        "        dpT_block = tl.dot(V_block, tl.trans(dO_block)).to(tl.float32)\n",
        "\n",
        "        #dS = P * (dP - Delta), so dS^T = P^T * (dP^T - Delta^T)\n",
        "\n",
        "        dS_T_block = P_T_block * (dpT_block - Di[None, :])\n",
        "        dS_T_block = dS_T_block.to(tl.float16)\n",
        "\n",
        "        #dK_new = dK_old + dS^T x Q\n",
        "        dK_block += softmax_scale * tl.dot(dS_T_block, tl.trans(qT_block))\n",
        "        # Increment pointers.\n",
        "        curr_q += BLOCK_Q\n",
        "        qT_ptrs += BLOCK_Q * stride_seq\n",
        "        dO_ptrs += BLOCK_Q * stride_seq\n",
        "\n",
        "    # Write back dV.\n",
        "    dV_block_ptrs = dV + offs_kv[:, None] * stride_seq + offs_dim[None, :] * stride_dim\n",
        "    tl.store(dV_block_ptrs, dV_block)\n",
        "\n",
        "    # Write back dK.\n",
        "    dK_block_ptrs = dK + offs_kv[:, None] * stride_seq + offs_dim[None, :] * stride_dim\n",
        "    tl.store(dK_block_ptrs, dK_block)\n"
      ],
      "metadata": {
        "id": "UidjlfhnDfTc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TritonAttention(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, Q, K, V, causal, softmax_scale):\n",
        "        HEAD_DIM_Q, HEAD_DIM_K = Q.shape[-1], K.shape[-1]\n",
        "        HEAD_DIM_V = V.shape[-1]\n",
        "\n",
        "        BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM = Q.shape\n",
        "\n",
        "        assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_K == HEAD_DIM_V\n",
        "\n",
        "        O = torch.empty_like(Q)\n",
        "        stage = 3 if causal else 1\n",
        "\n",
        "        grid = lambda args: (\n",
        "            triton.cdiv(SEQ_LEN, args[\"BLOCK_SIZE_Q\"]),\n",
        "            BATCH_SIZE * NUM_HEADS,\n",
        "            1,\n",
        "        )\n",
        "\n",
        "        # M is the logsumexp for the backward pass, one for each query\n",
        "        M = torch.empty(\n",
        "            (BATCH_SIZE, NUM_HEADS, SEQ_LEN), device=Q.device, dtype=torch.float32\n",
        "        )\n",
        "\n",
        "        _attn_fwd[grid](\n",
        "            Q=Q,\n",
        "            K=K,\n",
        "            V=V,\n",
        "            softmax_scale=softmax_scale,\n",
        "            M=M,\n",
        "            O=O,\n",
        "            stride_Q_batch=Q.stride(0),\n",
        "            stride_Q_head=Q.stride(1),\n",
        "            stride_Q_seq=Q.stride(2),\n",
        "            stride_Q_dim=Q.stride(3),\n",
        "            stride_K_batch=K.stride(0),\n",
        "            stride_K_head=K.stride(1),\n",
        "            stride_K_seq=K.stride(2),\n",
        "            stride_K_dim=K.stride(3),\n",
        "            stride_V_batch=V.stride(0),\n",
        "            stride_V_head=V.stride(1),\n",
        "            stride_V_seq=V.stride(2),\n",
        "            stride_V_dim=V.stride(3),\n",
        "            stride_O_batch=O.stride(0),\n",
        "            stride_O_head=O.stride(1),\n",
        "            stride_O_seq=O.stride(2),\n",
        "            stride_O_dim=O.stride(3),\n",
        "            BATCH_SIZE=Q.shape[0],\n",
        "            NUM_HEADS=Q.shape[1],\n",
        "            SEQ_LEN=Q.shape[2],\n",
        "            HEAD_DIM=HEAD_DIM_K,\n",
        "            STAGE=stage,\n",
        "        )\n",
        "\n",
        "        ctx.save_for_backward(Q, K, V, O, M)\n",
        "        ctx.grid = grid\n",
        "        ctx.softmax_scale = softmax_scale\n",
        "        ctx.HEAD_DIM = HEAD_DIM_K\n",
        "        ctx.causal = causal\n",
        "        return O\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, dO):\n",
        "        Q, K, V, O, M = ctx.saved_tensors\n",
        "\n",
        "        assert dO.is_contiguous()\n",
        "        assert Q.stride() == K.stride() == V.stride() == O.stride() == dO.stride()\n",
        "        dQ = torch.empty_like(Q)\n",
        "        dK = torch.empty_like(K)\n",
        "        dV = torch.empty_like(V)\n",
        "\n",
        "        BATCH_SIZE, NUM_HEADS, SEQ_LEN = Q.shape[:3]\n",
        "        NUM_WARPS, NUM_STAGES = 4, 3\n",
        "        BLOCK_SIZE_MICRO, BLOCK_SIZE_MACRO = 32, 128\n",
        "\n",
        "        preprocess_grid = (SEQ_LEN // BLOCK_SIZE_MACRO, BATCH_SIZE * NUM_HEADS)\n",
        "        D = torch.empty_like(M)  # Shape: (BATCH_SIZE, NUM_HEADS, SEQ_LEN)\n",
        "\n",
        "        # Compute all the elements Di\n",
        "        _attn_bwd_preprocess[preprocess_grid](\n",
        "            O=O,\n",
        "            dO=dO,\n",
        "            D=D,\n",
        "            SEQ_LEN=SEQ_LEN,\n",
        "            BLOCK_SIZE_Q=BLOCK_SIZE_MACRO,\n",
        "            HEAD_DIM=ctx.HEAD_DIM,\n",
        "        )\n",
        "\n",
        "        grid = (SEQ_LEN // BLOCK_SIZE_MACRO, 1, BATCH_SIZE * NUM_HEADS)\n",
        "\n",
        "        stage = 3 if ctx.causal else 1\n",
        "\n",
        "        # Fix KV and iterate through all the Q blocks\n",
        "        _attn_bwd_dk_dv[grid](\n",
        "            Q=Q,\n",
        "            K=K,\n",
        "            V=V,\n",
        "            softmax_scale=ctx.softmax_scale,\n",
        "            dO=dO,\n",
        "            dQ=dQ,\n",
        "            dK=dK,\n",
        "            dV=dV,\n",
        "            M=M,\n",
        "            D=D,\n",
        "            stride_batch=Q.stride(0),\n",
        "            stride_head=Q.stride(1),\n",
        "            stride_seq=Q.stride(2),\n",
        "            stride_dim=Q.stride(3),\n",
        "            NUM_HEADS=NUM_HEADS,\n",
        "            SEQ_LEN=SEQ_LEN,\n",
        "            BLOCK_Q=BLOCK_SIZE_MICRO,\n",
        "            BLOCK_KV=BLOCK_SIZE_MACRO,\n",
        "            HEAD_DIM=ctx.HEAD_DIM,\n",
        "            STAGE=stage,\n",
        "            num_warps=NUM_WARPS,\n",
        "            num_stages=NUM_STAGES,\n",
        "        )\n",
        "\n",
        "        # Fix Q and iterate through all the KV block\n",
        "        _attn_bwd_dq[grid](\n",
        "            Q=Q,\n",
        "            K=K,\n",
        "            V=V,\n",
        "            softmax_scale=ctx.softmax_scale,\n",
        "            dO=dO,\n",
        "            dQ=dQ,\n",
        "            dK=dK,\n",
        "            dV=dV,\n",
        "            M=M,\n",
        "            D=D,\n",
        "            stride_batch=Q.stride(0),\n",
        "            stride_head=Q.stride(1),\n",
        "            stride_seq=Q.stride(2),\n",
        "            stride_dim=Q.stride(3),\n",
        "            NUM_HEADS=NUM_HEADS,\n",
        "            SEQ_LEN=SEQ_LEN,\n",
        "            BLOCK_Q=BLOCK_SIZE_MACRO,\n",
        "            BLOCK_KV=BLOCK_SIZE_MICRO,\n",
        "            HEAD_DIM=ctx.HEAD_DIM,\n",
        "            STAGE=stage,\n",
        "            num_warps=NUM_WARPS,\n",
        "            num_stages=NUM_STAGES,\n",
        "        )\n",
        "\n",
        "        return dQ, dK, dV, None, None"
      ],
      "metadata": {
        "id": "6m6WPRYHEaX6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_op(BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM, causal, dtype=torch.float16):\n",
        "    Q = (\n",
        "        torch.empty(\n",
        "            (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=dtype, device=\"cuda\"\n",
        "        )\n",
        "        .normal_(mean=0.0, std=0.5)\n",
        "        .requires_grad_()\n",
        "    )\n",
        "    K = (\n",
        "        torch.empty(\n",
        "            (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=dtype, device=\"cuda\"\n",
        "        )\n",
        "        .normal_(mean=0.0, std=0.5)\n",
        "        .requires_grad_()\n",
        "    )\n",
        "    V = (\n",
        "        torch.empty(\n",
        "            (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=dtype, device=\"cuda\"\n",
        "        )\n",
        "        .normal_(mean=0.0, std=0.5)\n",
        "        .requires_grad_()\n",
        "    )\n",
        "\n",
        "    softmax_scale = 1 / (HEAD_DIM**0.5)\n",
        "    dO = torch.randn_like(Q)\n",
        "\n",
        "    # reference implementation\n",
        "    MASK = torch.tril(torch.ones((SEQ_LEN, SEQ_LEN), device=\"cuda\"))\n",
        "    P = torch.matmul(Q, K.transpose(2, 3)) * softmax_scale\n",
        "    if causal:\n",
        "        P[:, :, MASK == 0] = float(\"-inf\")\n",
        "    P = torch.softmax(P.float(), dim=-1).half()\n",
        "    ref_O = torch.matmul(P, V)\n",
        "    ref_O.backward(dO)\n",
        "    ref_dV, V.grad = V.grad.clone(), None\n",
        "    ref_dK, K.grad = K.grad.clone(), None\n",
        "    ref_dQ, Q.grad = Q.grad.clone(), None\n",
        "\n",
        "    # triton implementation\n",
        "    tri_out = TritonAttention.apply(Q, K, V, causal, softmax_scale).half()\n",
        "    tri_out.backward(dO)\n",
        "    tri_dV, V.grad = V.grad.clone(), None\n",
        "    tri_dK, K.grad = K.grad.clone(), None\n",
        "    tri_dQ, Q.grad = Q.grad.clone(), None\n",
        "\n",
        "    # compare\n",
        "    rtol = 0.0\n",
        "    atol = 1e-2\n",
        "    assert torch.allclose(ref_O, tri_out, atol=atol, rtol=rtol)\n",
        "    assert torch.allclose(ref_dK, tri_dK, atol=atol, rtol=rtol)\n",
        "    assert torch.allclose(ref_dV, tri_dV, atol=atol, rtol=rtol)\n",
        "    assert torch.allclose(ref_dQ, tri_dQ, atol=atol, rtol=rtol)\n"
      ],
      "metadata": {
        "id": "Z6HSALlODvNy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Trying simple params\")\n",
        "try:\n",
        "    # Minimal sequence length, HEAD_DIM=64 for the assertion\n",
        "    test_op(BATCH_SIZE=1, NUM_HEADS=1, SEQ_LEN=128, HEAD_DIM=64, causal=True)\n",
        "    test_op(BATCH_SIZE=1, NUM_HEADS=1, SEQ_LEN=128, HEAD_DIM=64, causal=False)\n",
        "    print(\"PASSED with very simple parameters (SEQ_LEN=128, HEADS=1)\")\n",
        "\n",
        "    # Check if SEQ_LEN=512 barrier exists\n",
        "    test_op(BATCH_SIZE=1, NUM_HEADS=1, SEQ_LEN=512, HEAD_DIM=64, causal=True)\n",
        "    test_op(BATCH_SIZE=1, NUM_HEADS=1, SEQ_LEN=512, HEAD_DIM=64, causal=False)\n",
        "    print(\"PASSED with simple parameters (SEQ_LEN=512, HEADS=1)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed even with simpler parameters: {e}\")\n",
        "\n",
        "\n",
        "print(\"\\nRetrying original target params\")\n",
        "try:\n",
        "    test_op(BATCH_SIZE=1, NUM_HEADS=2, SEQ_LEN=1024, HEAD_DIM=64, causal=True)\n",
        "    test_op(BATCH_SIZE=1, NUM_HEADS=2, SEQ_LEN=1024, HEAD_DIM=64, causal=False)\n",
        "    print(\"PASSED (BATCH=1, HEADS=2, SEQ_LEN=1024)\")\n",
        "except Exception as e:\n",
        "     print(f\"Still failing with Option 3 parameters: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFxPMyrI4cxA",
        "outputId": "e646b744-7627-45d1-d068-f68b5412b973"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying very simple parameters...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:823: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:180.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PASSED with very simple parameters (SEQ_LEN=128, HEADS=1)\n",
            "PASSED with simple parameters (SEQ_LEN=512, HEADS=1)\n",
            "\n",
            "Retrying original target parameters from Option 3...\n",
            "PASSED Option 3 (BATCH=1, HEADS=2, SEQ_LEN=1024)\n"
          ]
        }
      ]
    }
  ]
}
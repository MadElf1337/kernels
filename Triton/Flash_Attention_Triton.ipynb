{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO9buOvMJ6p0w24phjfYKJs"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUqV5vX2pU78"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import triton\n",
        "import triton.language as tl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def _attn_fwd_inner(\n",
        "        O_block,\n",
        "        norm_factor,\n",
        "        running_max,\n",
        "        Q_block_ptr,\n",
        "        K_block_ptr,\n",
        "        V_block_ptr,\n",
        "        block_index_q,\n",
        "        softmax_scale,\n",
        "        block_size_q: tl.constexpr,\n",
        "        block_size_kv: tl.constexpr,\n",
        "        stage: tl.constexpr,\n",
        "        block_query_offset: tl.constexpr,\n",
        "        block_kv_offset: tl.constexpr,\n",
        "        seq_len: tl.constexpr,\n",
        "):\n",
        "    if stage==1:\n",
        "        low, high = 0, block_index_q*block_size_q\n",
        "    elif stage==2:\n",
        "        low, high = block_index_q*block_size_q, (block_index_q+1)*block_size_q\n",
        "        low = tl.multiple_of(low, block_size_q)\n",
        "    else: #Non autoregressive attention\n",
        "        low, high = 0, seq_len\n",
        "\n",
        "    K_block_ptr = tl.advance(K_block_ptr, (0, low))\n",
        "    V_block_ptr = tl.advance(V_block_ptr, (low, 0))\n",
        "\n",
        "    for start_kv in range(low, high, block_size_kv):\n",
        "        start_kv = tl.multiple_of(start_kv, block_size_kv)\n",
        "        K_block = tl.load(K_block_ptr)\n",
        "        QK_block = tl.dot(Q_block_ptr, K_block)\n",
        "\n",
        "        if stage==2:\n",
        "            mask = block_query_offset[:, None] >= (start_kv + block_kv_offset[None, :])\n",
        "            QK_block = QK_block * softmax_scale + tl.where(mask, 0, -1.0e6)\n",
        "            running_max_ = tl.maximum(running_max, tl.max(QK_block, 1))\n",
        "            QK_block -= running_max_[:, None]\n",
        "        else:\n",
        "            running_max_ = tl.maximum(running_max, tl.max(QK_block, 1)*softmax_scale)\n",
        "            QK_block = QK_block*softmax_scale - running_max_[:, None]\n",
        "\n",
        "        P_block = tl.math.exp(QK_block)\n",
        "\n",
        "        norm_factor_ = tl.sum(P_block, 1)\n",
        "\n",
        "        alpha = tl.math.exp(running_max - running_max_)\n",
        "\n",
        "        norm_factor = norm_factor*alpha + norm_factor_\n",
        "\n",
        "        V_block = tl.load(V_block_ptr)\n",
        "\n",
        "        P_block = P_block.to(tl.float16)\n",
        "\n",
        "        O_block = O_block + alpha[:, None]\n",
        "        O_block = tl.dot(P_block, V_block, O_block)\n",
        "\n",
        "        running_max = running_max_\n",
        "\n",
        "        V_block_ptr = tl.advance(V_block_ptr, (block_size_kv, 0))\n",
        "        K_block_ptr = tl.advance(K_block_ptr, (0, block_size_kv))\n",
        "\n",
        "    return O_block, norm_factor, running_max"
      ],
      "metadata": {
        "id": "Ovm_wSte7zGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.autotune(\n",
        "    [\n",
        "        triton.Config(\n",
        "            {\"BLOCK_SIZE_Q\": block_size_q, \"BLOCK_SIZE_KV\": block_size_kv},\n",
        "            num_stages=num_stages,\n",
        "            num_warps=num_warps,\n",
        "        )\n",
        "        for block_size_q in [64, 128]\n",
        "        for block_size_kv in [32, 64]\n",
        "        for num_stages in ([3, 4, 7])\n",
        "        for num_warps in [2, 4]\n",
        "    ],\n",
        "    key=[\"seq_len\", \"head_dim\"],\n",
        ")\n",
        "@triton.jit\n",
        "def _attn_fwd(\n",
        "        Q,\n",
        "        K,\n",
        "        V,\n",
        "        softmax_scale,\n",
        "        O,\n",
        "        M,\n",
        "        stride_Q_batch,\n",
        "        stride_Q_head,\n",
        "        stride_Q_seq,\n",
        "        stride_Q_dim,\n",
        "        stride_K_batch,\n",
        "        stride_K_head,\n",
        "        stride_K_seq,\n",
        "        stride_K_dim,\n",
        "        stride_V_batch,\n",
        "        stride_V_head,\n",
        "        stride_V_seq,\n",
        "        stride_V_dim,\n",
        "        stride_O_batch,\n",
        "        stride_O_head,\n",
        "        stride_O_seq,\n",
        "        stride_O_dim,\n",
        "        batch_size,\n",
        "        n_heads: tl.constexpr,\n",
        "        seq_len: tl.constexpr,\n",
        "        head_dim: tl.constexpr,\n",
        "        block_size_q: tl.constexpr,\n",
        "        block_size_kv: tl.constexpr,\n",
        "        stage: tl.constexpr,\n",
        "):\n",
        "    tl.static_assert(block_size_kv <= head_dim)\n",
        "\n",
        "    block_index_q = tl.program_id(0)\n",
        "\n",
        "    index_batch_head = tl.program_id(1)\n",
        "\n",
        "    index_batch = index_batch_head // n_heads\n",
        "\n",
        "    index_head = index_batch_head % n_heads\n",
        "\n",
        "    qkv_offset = (\n",
        "        index_batch.to(tl.int64)*stride_Q_batch + index_head.to(tl.int64)*stride_Q_head\n",
        "    )\n",
        "\n",
        "    Q_block_ptr = tl.make_block_ptr( #Q[index_batch, index_head, block_index_q*block_size_q:, :]\n",
        "        base=Q + qkv_offset,\n",
        "        shape=(seq_len, head_dim),\n",
        "        strides=(stride_Q_seq, stride_Q_dim),\n",
        "        offsets=(block_index_q*block_size_q, 0),\n",
        "        block_shape=(block_size_q, head_dim),\n",
        "        order=(1,0),\n",
        "    )\n",
        "\n",
        "    V_block_ptr = tl.make_block_ptr( #V[index_batch, index_head, :, :]\n",
        "        base=V + qkv_offset,\n",
        "        shape=(seq_len, head_dim),\n",
        "        strides=(stride_V_seq, stride_V_dim),\n",
        "        offsets=(0, 0),\n",
        "        block_shape=(block_size_kv, head_dim),\n",
        "        order=(0,1),\n",
        "    )\n",
        "\n",
        "    K_block_ptr = tl.make_block_ptr(\n",
        "        base=K + qkv_offset,\n",
        "        shape=(head_dim, seq_len),\n",
        "        strides=(stride_K_dim, stride_K_seq),\n",
        "        offsets=(0, 0),\n",
        "        block_shape=(head_dim, block_size_kv),\n",
        "        order=(1,0),\n",
        "    )\n",
        "\n",
        "    O_block_ptr = tl.make_block_ptr(\n",
        "        base=O + qkv_offset,\n",
        "        shape=(seq_len, head_dim),\n",
        "        strides=(stride_O_seq, stride_O_dim),\n",
        "        offsets=(block_index_q*block_size_q, 0),\n",
        "        block_shape=(block_size_q, head_dim),\n",
        "        order=(1,0)\n",
        "    )\n",
        "\n",
        "    block_query_offset = block_index_q * block_size_q + tl.arange(0, block_size_q)\n",
        "\n",
        "    block_kv_offset = tl.arange(0, block_size_kv)\n",
        "\n",
        "    running_max = tl.zeros([block_size_q], dtype=tl.float32) - float('inf')\n",
        "\n",
        "    norm_factor = tl.zeros([block_size_q], dtype=tl.float32) + 1.0\n",
        "\n",
        "    O_block = tl.zeros([block_size_q, head_dim], dtype=tl.float32)\n",
        "\n",
        "    Q_block = tl.load(Q_block_ptr)\n",
        "\n",
        "    if stage==1 or stage==3:\n",
        "        O_block, norm_factor, running_max = _attn_fwd_inner(\n",
        "            O_block,\n",
        "            norm_factor,\n",
        "            running_max,\n",
        "            Q_block_ptr,\n",
        "            K_block_ptr,\n",
        "            V_block_ptr,\n",
        "            block_index_q,\n",
        "            softmax_scale,\n",
        "            block_size_q,\n",
        "            block_size_kv,\n",
        "            4-stage,\n",
        "            block_query_offset,\n",
        "            block_kv_offset,\n",
        "            seq_len,\n",
        "        )\n",
        "\n",
        "    if stage==3:\n",
        "        #Right Diagonal Calculcation for causal\n",
        "        O_block, norm_factor, running_max = _attn_fwd_inner(\n",
        "            O_block,\n",
        "            norm_factor,\n",
        "            running_max,\n",
        "            Q_block_ptr,\n",
        "            K_block_ptr,\n",
        "            V_block_ptr,\n",
        "            block_index_q,\n",
        "            softmax_scale,\n",
        "            block_size_q,\n",
        "            block_size_kv,\n",
        "            2,\n",
        "            block_query_offset,\n",
        "            block_kv_offset,\n",
        "            seq_len,\n",
        "        )\n",
        "\n",
        "    #Logsumexp trick for backward\n",
        "    running_max += tl.math.log(norm_factor)\n",
        "\n",
        "    O_block = O_block / norm_factor[:, None]\n",
        "\n",
        "    m_ptrs = M + index_batch_head * seq_len + block_query_offset\n",
        "\n",
        "    tl.store(m_ptrs, running_max)\n",
        "\n",
        "    tl.store(O_block_ptr, O_block.to(O.type.element_ty))"
      ],
      "metadata": {
        "id": "OTGxAja66_99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def _attn_bwd_preprocess(\n",
        "        O,\n",
        "        dO,\n",
        "        D,\n",
        "        seq_len,\n",
        "        block_size_q: tl.constexpr,\n",
        "        head_dim: tl.constexpr,\n",
        "):\n",
        "    block_index_q = tl.program_id(0)\n",
        "\n",
        "    q_offset = block_index_q * block_size_q + tl.arange(0, block_size_q)\n",
        "\n",
        "    index_batch_head = tl.program_id(1)\n",
        "\n",
        "    dim_offset = tl.arange(0, head_dim)\n",
        "\n",
        "    O_block = tl.load(\n",
        "        O\n",
        "        + index_batch_head * head_dim * seq_len\n",
        "        + q_offset[:, None] * head_dim\n",
        "        + dim_offset[None, :]\n",
        "    )\n",
        "\n",
        "    dO_block = tl.load(\n",
        "        dO\n",
        "        + index_batch_head * head_dim * seq_len\n",
        "        + q_offset[:, None] * head_dim\n",
        "        + dim_offset[None, :]\n",
        "    ).to(tl.float32)\n",
        "\n",
        "    D_block = tl.sum(dO_block * O_block, axis=1)\n",
        "\n",
        "    D_block_ptrs = D + index_batch_head * seq_len + q_offset\n",
        "\n",
        "    tl.store(D_block_ptrs, D_block)"
      ],
      "metadata": {
        "id": "9oqtXnRaUimt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def _attn_bwd_dq(\n",
        "    Q,\n",
        "    K,\n",
        "    V,\n",
        "    softmax_scale,\n",
        "    dO,\n",
        "    dQ,\n",
        "    dK,\n",
        "    dV,\n",
        "    M,\n",
        "    D,\n",
        "    stride_batch,\n",
        "    stride_head,\n",
        "    stride_seq,\n",
        "    stride_dim,\n",
        "    n_heads,\n",
        "    seq_len,\n",
        "    BLOCK_Q: tl.constexpr,\n",
        "    BLOCK_KV: tl.constexpr,\n",
        "    HEAD_DIM: tl.constexpr,\n",
        "    STAGE: tl.constexpr,\n",
        "):\n",
        "    index_batch_head = tl.program_id(2)\n",
        "    index_batch = index_batch_head // n_heads\n",
        "    index_head = index_batch_head % n_heads\n",
        "    offset_batch_head = (stride_batch * index_batch + stride_head * index_head).to(\n",
        "        tl.int64\n",
        "    )\n",
        "    # This is the offset that allows us to select the right sequence given the batch and head.\n",
        "    offset_batch_head_seq = (index_batch_head * seq_len).to(tl.int64)\n",
        "\n",
        "\n",
        "    Q += offset_batch_head\n",
        "    K += offset_batch_head\n",
        "    V += offset_batch_head\n",
        "    dO += offset_batch_head\n",
        "    dQ += offset_batch_head\n",
        "    dK += offset_batch_head\n",
        "    dV += offset_batch_head\n",
        "\n",
        "    # Make sure the pointers are in the right place w.r.t batch, head and sequence\n",
        "    M += offset_batch_head_seq\n",
        "    D += offset_batch_head_seq\n",
        "\n",
        "    # load scales\n",
        "    offs_dim = tl.arange(0, HEAD_DIM)\n",
        "\n",
        "    index_block_kv = tl.program_id(0)\n",
        "\n",
        "    start_q = index_block_kv * BLOCK_Q\n",
        "    offs_q = start_q + tl.arange(0, BLOCK_Q)\n",
        "\n",
        "    Q_block = tl.load(Q + offs_q[:, None] * stride_seq + offs_dim[None, :] * stride_dim)\n",
        "    dQ_block = tl.zeros([BLOCK_Q, HEAD_DIM], dtype=tl.float32)\n",
        "    dO_block = tl.load(\n",
        "        dO + offs_q[:, None] * stride_seq + offs_dim[None, :] * stride_dim\n",
        "    )\n",
        "\n",
        "    M_block = tl.load(M + offs_q)\n",
        "    M_block = M_block[:, None]\n",
        "\n",
        "    offs_kv = tl.arange(0, BLOCK_KV)\n",
        "\n",
        "    # We access the K and V as transposed blocks\n",
        "    kT_ptrs = K + offs_kv[None, :] * stride_seq + offs_dim[:, None] * stride_dim\n",
        "    vT_ptrs = V + offs_kv[None, :] * stride_seq + offs_dim[:, None] * stride_dim\n",
        "\n",
        "    Di = tl.load(D + offs_q)\n",
        "\n",
        "    curr_kv = 0\n",
        "    num_steps = seq_len // BLOCK_KV\n",
        "    for blk_idx in range(num_steps):\n",
        "        K_T_block = tl.load(kT_ptrs)\n",
        "        V_T_block = tl.load(vT_ptrs)\n",
        "        QK_block = softmax_scale * tl.dot(Q_block, K_T_block)\n",
        "        P_block = tl.math.exp(QK_block - M_block)\n",
        "\n",
        "        if STAGE == 3:\n",
        "            # Autoregressive masking.\n",
        "            offs_kv = curr_kv + tl.arange(0, BLOCK_KV)\n",
        "            mask_block = offs_q[:, None] >= offs_kv[None, :]\n",
        "            P_block = tl.where(mask_block, P_block, 0.0)\n",
        "\n",
        "        # Compute dP and dS.\n",
        "        dP_block = tl.dot(dO_block, V_T_block).to(tl.float32)\n",
        "        dS_block = P_block * (dP_block - Di[:, None])\n",
        "        dS_block = dS_block.to(tl.float16)\n",
        "        # Compute dQ.\n",
        "        dQ_block += softmax_scale * tl.dot(dS_block, tl.trans(K_T_block))\n",
        "        # Increment pointers.\n",
        "        curr_kv += BLOCK_KV\n",
        "        kT_ptrs += BLOCK_KV * stride_seq\n",
        "        vT_ptrs += BLOCK_KV * stride_seq\n",
        "\n",
        "    dQ_block_ptrs = dQ + offs_q[:, None] * stride_seq + offs_dim[None, :] * stride_dim\n",
        "    tl.store(dQ_block_ptrs, dQ_block)"
      ],
      "metadata": {
        "id": "IKl9yH0lUIeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def _attn_bwd_dk_dv(\n",
        "        Q,\n",
        "        K,\n",
        "        V,\n",
        "        softmax_scale,\n",
        "        dO,\n",
        "        dQ,\n",
        "        dK,\n",
        "        dV,\n",
        "        M,\n",
        "        D,\n",
        "        stride_batch,\n",
        "        stride_head,\n",
        "        stride_seq,\n",
        "        stride_dim,\n",
        "        n_heads,\n",
        "        seq_len,\n",
        "        block_q: tl.constexpr,\n",
        "        block_kv: tl.constexpr,\n",
        "        head_dim: tl.constexpr,\n",
        "        stage: tl.constexpr,\n",
        "):\n",
        "    index_batch_head = tl.program_id(2)\n",
        "    index_batch = index_batch_head // n_heads\n",
        "    index_head = index_batch_head % n_heads\n",
        "\n",
        "    offset_batch_head = (stride_batch * index_batch + stride_head * index_head).to(tl.int64)\n",
        "\n",
        "    offset_batch_head_seq = (index_batch_head * seq_len).to(tl.int64)\n",
        "\n",
        "    Q += offset_batch_head\n",
        "    K += offset_batch_head\n",
        "    V += offset_batch_head\n",
        "    dO += offset_batch_head\n",
        "    dQ += offset_batch_head\n",
        "    dK += offset_batch_head\n",
        "    dV += offset_batch_head\n",
        "\n",
        "    M += offset_batch_head_seq\n",
        "    D += offset_batch_head_seq\n",
        "\n",
        "    dim_offset = tl.arange(0, head_dim)\n",
        "\n",
        "    index_block_kv = tl.program_id(0)\n",
        "    start_kv = index_block_kv * block_kv\n",
        "\n",
        "    kv_offset = start_kv + tl.arange(0, block_kv)\n",
        "\n",
        "    dV_block = tl.zeros([block_kv, head_dim], dtype=tl.float32)\n",
        "    dK_block = tl.zeros([head_dim, block_kv], dtype=tl.float32)\n",
        "\n",
        "    K_block = tl.load(\n",
        "        K + kv_offset[:, None] * stride_seq + dim_offset[None, :] * stride_dim\n",
        "    )\n",
        "\n",
        "    V_block = tl.load(\n",
        "        V + kv_offset[:, None] * stride_seq + dim_offset[None, :] * stride_dim\n",
        "    )\n",
        "\n",
        "    q_offset = tl.arange(0, block_q)\n",
        "\n",
        "    dO_block = tl.load(\n",
        "        dO + kv_offset[:, None] * stride_seq + dim_offset[None, :] * stride_dim\n",
        "    )\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "4yyTPsXKNy0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_op(batch_size, n_heads, seq_len, head_dim, causal, dtype=torch.float16):\n",
        "    Q = (torch.empty((batch_size, n_heads, seq_len, head_dim), dtype=dtype, device='cuda').normal(mean=0.0, std=0.5).requires_grad())\n",
        "    K = (torch.empty((batch_size, n_heads, seq_len, head_dim), dtype=dtype, device='cuda').normal(mean=0.0, std=0.5).requires_grad())\n",
        "    V = (torch.empty((batch_size, n_heads, seq_len, head_dim), dtype=dtype, device='cuda').normal(mean=0.0, std=0.5).requires_grad())\n",
        "    softmax_scale = 1/(head_dim**0.5)\n",
        "    d0 = torch.randn_like(Q)\n",
        "\n",
        "    mask = torch.tril(torch.ones((seq_len, seq_len), device='cuda'))\n",
        "    P = torch.matmul(Q, K.transpose(-1, -2))*softmax_scale\n",
        "    if causal:\n",
        "        P[:, :, mask==0] = float('-inf')\n",
        "    P = torch.softmax(P.float(), dim=-1).half()\n",
        "    ref_0=torch.matmul(P,V)\n",
        "    ref_0.backward(d0)\n",
        "    ref_dV, V.grad = V.grad.clone(), None\n",
        "    ref_dK, K.grad = K.grad.clone(), None\n",
        "    ref_dQ, Q.grad = Q.grad.clone(), None\n",
        "\n",
        "    tri_out = TritonAttention.apply(Q, K, V, causal, softmax_scale).half()\n",
        "    tri_out.backward(d0)\n",
        "    tri_dV, V.grad = V.grad.clone(), None\n",
        "    tri_dK, K.grad = K.grad.clone(), None\n",
        "    tri_dQ, Q.grad = Q.grad.clone(), None\n",
        "\n",
        "    rtol = 0.0\n",
        "    atol = 1e-2\n",
        "    assert torch.allclose(ref_0, tri_out, rtol=rtol, atol=atol)\n",
        "    assert torch.allclose(ref_dV, tri_dV, rtol=rtol, atol=atol)\n",
        "    assert torch.allclose(ref_dK, tri_dK, rtol=rtol, atol=atol)\n",
        "    assert torch.allclose(ref_dQ, tri_dQ, rtol=rtol, atol=atol)"
      ],
      "metadata": {
        "id": "5VnHxIqUpg-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TritonAttention(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, Q, K, V, causal, softmax_scale):\n",
        "        head_dim_q, head_dim_k = Q.shape[-1], K.shape[-1]\n",
        "        head_dim_v = V.shape[-1]\n",
        "        batch_size, n_heads, seq_len, head_dim = Q.shape\n",
        "\n",
        "        assert head_dim_q == head_dim_k and head_dim_k == head_dim_v\n",
        "\n",
        "        O = torch.empty_like(Q)\n",
        "        stage = 3 if causal else 1\n",
        "\n",
        "        grid = lambda args: (\n",
        "            triton.cdiv(seq_len, args['block_size_q']),\n",
        "            batch_size*n_heads,\n",
        "            1,\n",
        "        )\n",
        "\n",
        "        M = torch.empty((batch_size, n_heads, seq_len,), device=Q.device, dtype=torch.float32)\n",
        "\n",
        "        _attn_fwd[grid](\n",
        "            Q=Q,\n",
        "            K=K,\n",
        "            V=V,\n",
        "            softmax_scale=softmax_scale,\n",
        "            O=O,\n",
        "            M=M,\n",
        "            stride_Q_batch=Q.stride(0),\n",
        "            stride_Q_head=Q.stride(1),\n",
        "            stride_Q_seq=Q.stride(2),\n",
        "            stride_Q_dim=Q.stride(3),\n",
        "            stride_K_batch=K.stride(0),\n",
        "            stride_K_head=K.stride(1),\n",
        "            stride_K_seq=K.stride(2),\n",
        "            stride_K_dim=K.stride(3),\n",
        "            stride_V_batch=V.stride(0),\n",
        "            stride_V_head=V.stride(1),\n",
        "            stride_V_seq=V.stride(2),\n",
        "            stride_V_dim=V.stride(3),\n",
        "            stride_O_batch=O.stride(0),\n",
        "            stride_O_head=O.stride(1),\n",
        "            stride_O_seq=O.stride(2),\n",
        "            stride_O_dim=O.stride(3),\n",
        "            batch_size=Q.shape(0),\n",
        "            n_heads=Q.shape(1),\n",
        "            seq_len=Q.shape(2),\n",
        "            head_dim=head_dim_k,\n",
        "            stage=stage\n",
        "        )\n",
        "\n",
        "        ctx.save_for_backward(Q, K, V, O, M)\n",
        "        ctx.grid = grid\n",
        "        ctx.softmax_scale = softmax_scale\n",
        "        ctx.head_dim = head_dim_k\n",
        "        ctx.causal = causal\n",
        "        return O\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, dO):\n",
        "        Q, K, V, O, M = ctx.saved_tensors\n",
        "        assert dO.is_contiguous()\n",
        "        assert Q.stride() == K.stride() == V.stride() == O.stride() == dO.stride()\n",
        "        dQ = torch.empty_like(Q)\n",
        "        dK = torch.empty_like(K)\n",
        "        dV = torch.empty_like(V)\n",
        "\n",
        "        batch_size, n_heads, seq_len = Q.shape[:3]\n",
        "        n_warps, n_stages = 4, 3\n",
        "        block_size_micro, block_size_macro = 32, 128\n",
        "\n",
        "        preprocess_grid = (seq_len // block_size_macro, batch_size*n_heads)\n",
        "        D = torch.empty_like(M)\n",
        "\n",
        "        _attn_bwd_preprocess[preprocess_grid](\n",
        "            O=O,\n",
        "            dO=dO,\n",
        "            D=D,\n",
        "            seq_len=seq_len,\n",
        "            block_size_q=block_size_macro,\n",
        "            head_dim=ctx.head_dim,\n",
        "        )\n",
        "\n",
        "        grid = (seq_len // block_size_macro, 1, batch_size * n_heads)\n",
        "\n",
        "        stage = 3 if ctx.causal else 1\n",
        "\n",
        "        _attn_bwd_dk_dv[grid](\n",
        "            Q=Q,\n",
        "            K=K,\n",
        "            V=V,\n",
        "            softmax_scale=ctx.softmax_scale,\n",
        "            dO=dO,\n",
        "            dK=dK,\n",
        "            dV=dV,\n",
        "            M=M,\n",
        "            D=D,\n",
        "            stride_batch=Q.stride(0),\n",
        "            stride_head=Q.stride(1),\n",
        "            stride_seq=Q.stride(2),\n",
        "            stride_dim=Q.stride(3),\n",
        "            n_heads=n_heads,\n",
        "            seq_len=seq_len,\n",
        "            block_q=block_size_micro,\n",
        "            block_kv=block_size_macro,\n",
        "            head_dim=ctx.head_dim,\n",
        "            stage=stage,\n",
        "            n_warps=n_warps,\n",
        "            n_stages=n_stages,\n",
        "        )"
      ],
      "metadata": {
        "id": "0A9CiHbWqiO1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
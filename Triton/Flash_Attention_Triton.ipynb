{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP9Snauh1FwPUOYdwwSx2T+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUqV5vX2pU78"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import triton\n",
        "import triton.language as tl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_op(batch_size, n_heads, seq_len, head_dim, causal, dtype=torch.float16):\n",
        "    Q = (torch.empty((batch_size, n_heads, seq_len, head_dim), dtype=dtype, device='cuda').normal(mean=0.0, std=0.5).requires_grad())\n",
        "    K = (torch.empty((batch_size, n_heads, seq_len, head_dim), dtype=dtype, device='cuda').normal(mean=0.0, std=0.5).requires_grad())\n",
        "    V = (torch.empty((batch_size, n_heads, seq_len, head_dim), dtype=dtype, device='cuda').normal(mean=0.0, std=0.5).requires_grad())\n",
        "    softmax_scale = 1/(head_dim**0.5)\n",
        "    d0 = torch.randn_like(Q)\n",
        "\n",
        "    mask = torch.tril(torch.ones((seq_len, seq_len), device='cuda'))\n",
        "    P = torch.matmul(Q, K.transpose(-1, -2))*softmax_scale\n",
        "    if causal:\n",
        "        P[:, :, mask==0] = float('-inf')\n",
        "    P = torch.softmax(P.float(), dim=-1).half()\n",
        "    ref_0=torch.matmul(P,V)\n",
        "    ref_0.backward(d0)\n",
        "    ref_dV, V.grad = V.grad.clone(), None\n",
        "    ref_dK, K.grad = K.grad.clone(), None\n",
        "    ref_dQ, Q.grad = Q.grad.clone(), None\n",
        "\n",
        "    tri_out = TritonAttention.apply(Q, K, V, causal, softmax_scale).half()\n",
        "    tri_out.backward(d0)\n",
        "    tri_dV, V.grad = V.grad.clone(), None\n",
        "    tri_dK, K.grad = K.grad.clone(), None\n",
        "    tri_dQ, Q.grad = Q.grad.clone(), None\n",
        "\n",
        "    rtol = 0.0\n",
        "    atol = 1e-2\n",
        "    assert torch.allclose(ref_0, tri_out, rtol=rtol, atol=atol)\n",
        "    assert torch.allclose(ref_dV, tri_dV, rtol=rtol, atol=atol)\n",
        "    assert torch.allclose(ref_dK, tri_dK, rtol=rtol, atol=atol)\n",
        "    assert torch.allclose(ref_dQ, tri_dQ, rtol=rtol, atol=atol)"
      ],
      "metadata": {
        "id": "5VnHxIqUpg-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TritonAttention(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, Q, K, V, causal, softmax_scale):\n",
        "        head_dim_q, head_dim_k = Q.shape[-1], K.shape[-1]\n",
        "        head_dim_v = V.shape[-1]\n",
        "        batch_size, n_heads, seq_len, head_dim = Q.shape\n",
        "\n",
        "        assert head_dim_q == head_dim_k and head_dim_k == head_dim_v\n",
        "\n",
        "        O = torch.empty_like(Q)\n",
        "        stage = 3 if causal else 1\n",
        "\n",
        "        grid = lambda args: (\n",
        "            triton.cdiv(seq_len, args['block_size_q']),\n",
        "            batch_size*n_heads,\n",
        "            1,\n",
        "        )\n",
        "\n",
        "        M = torch.empty((batch_size, n_heads, seq_len,), device=Q.device, dtype=torch.float32)\n",
        "\n",
        "        _attn_fwd[grid](\n",
        "            Q=Q,\n",
        "            K=K,\n",
        "            V=V,\n",
        "            softmax_scale=softmax_scale,\n",
        "            O=O,\n",
        "            M=M,\n",
        "            stride_Q_batch=Q.stride(0),\n",
        "            stride_Q_head=Q.stride(1),\n",
        "            stride_Q_seq=Q.stride(2),\n",
        "            stride_Q_dim=Q.stride(3),\n",
        "            stride_K_batch=K.stride(0),\n",
        "            stride_K_head=K.stride(1),\n",
        "            stride_K_seq=K.stride(2),\n",
        "            stride_K_dim=K.stride(3),\n",
        "            stride_V_batch=V.stride(0),\n",
        "            stride_V_head=V.stride(1),\n",
        "            stride_V_seq=V.stride(2),\n",
        "            stride_V_dim=V.stride(3),\n",
        "            stride_O_batch=O.stride(0),\n",
        "            stride_O_head=O.stride(1),\n",
        "            stride_O_seq=O.stride(2),\n",
        "            stride_O_dim=O.stride(3),\n",
        "            batch_size=Q.shape(0),\n",
        "            n_heads=Q.shape(1),\n",
        "            seq_len=Q.shape(2),\n",
        "            head_dim=head_dim_k,\n",
        "            stage=stage\n",
        "        )\n",
        "\n",
        "        ctx.save_for_backward(Q, K, V, O, M)\n",
        "        ctx.grid = grid\n",
        "        ctx.softmax_scale = softmax_scale\n",
        "        ctx.head_dim = head_dim_k\n",
        "        ctx.causal = causal\n",
        "        return O"
      ],
      "metadata": {
        "id": "0A9CiHbWqiO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def _attn_fwd(\n",
        "        Q,\n",
        "        K,\n",
        "        V,\n",
        "        softmax_scale,\n",
        "        O,\n",
        "        M,\n",
        "        stride_Q_batch,\n",
        "        stride_Q_head,\n",
        "        stride_Q_seq,\n",
        "        stride_Q_dim,\n",
        "        stride_K_batch,\n",
        "        stride_K_head,\n",
        "        stride_K_seq,\n",
        "        stride_K_dim,\n",
        "        stride_V_batch,\n",
        "        stride_V_head,\n",
        "        stride_V_seq,\n",
        "        stride_V_dim,\n",
        "        stride_O_batch,\n",
        "        stride_O_head,\n",
        "        stride_O_seq,\n",
        "        stride_O_dim,\n",
        "        batch_size,\n",
        "        n_heads: tl.constexpr,\n",
        "        seq_len: tl.constexpr,\n",
        "        head_dim: tl.constexpr,\n",
        "        block_size_q: tl.constexpr,\n",
        "        block_size_kv: tl.constexpr,\n",
        "        stage: tl.constexpr,\n",
        "):\n",
        "    tl.static_assert(block_size_kv <= head_dim)\n",
        "\n",
        "    block_index_q = tl.program_id(0)\n",
        "\n",
        "    index_batch_head = tl.program_id(1)\n",
        "\n",
        "    index_batch = index_batch_head // n_heads\n",
        "\n",
        "    index_head = index_batch_head % n_heads\n",
        "\n",
        "    qkv_offset = (\n",
        "        index_batch.to(tl.int64)*stride_Q_batch + index_head.to(tl.int64)*stride_Q_head\n",
        "    )\n",
        "\n",
        "    Q_block_ptr = tl.make_block_ptr( #Q[index_batch, index_head, block_index_q*block_size_q:, :]\n",
        "        base=Q + qkv_offset,\n",
        "        shape=(seq_len, head_dim),\n",
        "        strides=(stride_Q_seq, stride_Q_dim),\n",
        "        offsets=(block_index_q*block_size_q, 0),\n",
        "        block_shape=(block_size_q, head_dim),\n",
        "        order=(1,0),\n",
        "    )\n",
        "\n",
        "    V_block_ptr = tl.make_block_ptr( #V[index_batch, index_head, :, :]\n",
        "        base=V + qkv_offset,\n",
        "        shape=(seq_len, head_dim),\n",
        "        strides=(stride_V_seq, stride_V_dim),\n",
        "        offsets=(0, 0),\n",
        "        block_shape=(block_size_kv, head_dim),\n",
        "        order=(0,1),\n",
        "    )\n",
        "\n",
        "    K_block_ptr = tl.make_block_ptr(\n",
        "        base=K + qkv_offset,\n",
        "        shape=(head_dim, seq_len),\n",
        "        strides=(stride_K_dim, stride_K_seq),\n",
        "        offsets=(0, 0),\n",
        "        block_shape=(head_dim, block_size_kv),\n",
        "        order=(1,0),\n",
        "    )\n",
        "\n",
        "    O_block_ptr = tl.make_block_ptr(\n",
        "        base=O + qkv_offset,\n",
        "        shape=(seq_len, head_dim),\n",
        "        strides=(stride_O_seq, stride_O_dim),\n",
        "        offsets=(block_index_q*block_size_q, 0),\n",
        "        block_shape=(block_size_q, head_dim),\n",
        "        order=(1,0)\n",
        "    )\n",
        "\n",
        "    block_query_offset = block_index_q * block_size_q + tl.arange(0, block_size_q)\n",
        "\n",
        "    block_kv_offset = tl.arange(0, block_size_kv)\n",
        "\n",
        "    running_max = tl.zeros([block_size_q], dtype=tl.float32) - float('inf')\n",
        "\n",
        "    norm_factor = tl.zeros([block_size_q], dtype=tl.float32) + 1.0\n",
        "\n",
        "    O_block = tl.zeros([block_size_q, head_dim], dtype=tl.float32)\n",
        "\n",
        "    Q_block = tl.load(Q_block_ptr)\n",
        "\n",
        "    if stage==1 or stage==3:\n",
        "        O_block, norm_factor, running_max = _attn_fwd_inner(\n",
        "            O_block,\n",
        "            norm_factor,\n",
        "            running_max,\n",
        "            Q_block_ptr,\n",
        "            K_block_ptr,\n",
        "            V_block_ptr,\n",
        "            block_index_q,\n",
        "            softmax_scale,\n",
        "            block_size_q,\n",
        "            block_size_kv,\n",
        "            4-stage,\n",
        "            block_query_offset,\n",
        "            block_kv_offset,\n",
        "            seq_len,\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "OTGxAja66_99",
        "outputId": "0bec9d53-2dc8-4ede-ef9c-cb5224342469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (<ipython-input-2-8fc5b08d6113>, line 26)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-8fc5b08d6113>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    n_heads: tl.constexpr\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def _attn_fwd_inner(\n",
        "        O_block,\n",
        "        norm_factor,\n",
        "        running_max,\n",
        "        Q_block_ptr,\n",
        "        K_block_ptr,\n",
        "        V_block_ptr,\n",
        "        block_index_q,\n",
        "        softmax_scale,\n",
        "        block_size_q: tl.constexpr,\n",
        "        block_size_kv: tl.constexpr,\n",
        "        stage: tl.constexpr,\n",
        "        block_query_offset: tl.constexpr,\n",
        "        block_kv_offset: tl.constexpr,\n",
        "        seq_len: tl.constexpr,\n",
        "):\n",
        "    if stage==1:\n",
        "        low, high = 0, block_index_q*block_size_q\n",
        "    elif stage==2:\n",
        "        low, high = block_index_q*block_size_q, (block_index_q+1)*block_size_q\n",
        "        low = tl.multiple_of(lo, block_size_q)\n",
        "    else: #Non autoregressive attention\n",
        "        low, high = 0, seq_len\n",
        "\n",
        "    K_block_ptr = tl.advance(K_block_ptr, (0, low))\n",
        "    V_block_ptr = tl.advance(V_block_ptr, (low, 0))\n",
        "\n",
        "    for start_kv in range(low, high, block_size_kv):\n",
        "        start_kv = tl.multiple_of(start_kv, block_size_kv)\n",
        "        K_block = tl.load(K_block_ptr)\n",
        "        QK_block = tl.dot(Q_block, K_block)\n",
        "\n",
        "        if stage==2:\n",
        "            mask = block_query_offset[:, None] >= (start_kv + block_kv_offset[None, :])\n",
        "            QK_block = QK_block * softmax_scale + tl.where(mask, 0, -1.0e6)\n",
        "            running_max_ = tl.maximum(running_max, tl.max(QK_block, 1))\n",
        "            QK_block -= running_max_[:, None]\n",
        "        else:\n",
        "            running_max_ = tl.maximum(running_max, tl.max(QK_block, 1)*softmax_scale)\n",
        "            QK_block = QK_block*softmax_scale - running_max_[:, None]\n",
        "\n",
        "        P_block = tl.math.exp(QK_block)\n",
        "\n",
        "        norm_factor_ = tl.sum(P_block, 1)\n",
        "\n",
        "        alpha = tl.math.exp(running_max - running_max_)\n",
        "\n",
        "        norm_factor = norm_factor*alpha + norm_factor_\n",
        "\n",
        "        V_block = tl.load(V_block_ptr)\n",
        "\n",
        "        P_block = P_block.to(tl.float16)\n",
        "\n",
        "        O_block = O_block + alpha[:, None]\n",
        "        O_block = tl.dot(P_block, V_block, O_block)\n",
        "\n",
        "        running_max = running_max_\n",
        "\n",
        "        V_block_ptr = tl.advance(V_block_ptr, (block_size_kv, 0))\n",
        "        K_block_ptr = tl.advance(K_block_ptr, (0, block_size_kv))\n",
        "\n",
        "    return O_block, norm_factor, running_max\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ovm_wSte7zGx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}